{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfv6T+T57pO6kILEWwncZf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keduog/LLM/blob/main/all_biology_Exam_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imksbFO6MsWC"
      },
      "outputs": [],
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx+1}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology multiple-choice question (MCQ) using Bloom's Revised Taxonomy.\n",
        "\n",
        "Use only one of the following levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond strictly in the format:\n",
        "Q1: [Level]\n",
        "Q2: [Level]\n",
        "...\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx+1}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification (Batched) ===\n",
        "difficulty_levels = []\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        lines = response.text.strip().splitlines()\n",
        "        for line in lines:\n",
        "            try:\n",
        "                diff = line.split(\":\")[1].strip()\n",
        "                difficulty_levels.append(diff if diff in [\"Easy\", \"Medium\", \"Difficult\"] else \"Unclear\")\n",
        "            except:\n",
        "                difficulty_levels.append(\"Unclear\")\n",
        "        # In case of mismatch in count, pad with \"Unclear\"\n",
        "        while len(difficulty_levels) < len(exam_df):\n",
        "            difficulty_levels.append(\"Unclear\")\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "        difficulty_levels += [\"API Error\"] * len(batch_df)\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels[:len(exam_df)]\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Batched) ===\n",
        "bloom_levels = []\n",
        "valid_levels = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_blooms_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        lines = response.text.strip().splitlines()\n",
        "        for line in lines:\n",
        "            try:\n",
        "                bloom = line.split(\":\")[1].strip()\n",
        "                bloom_levels.append(bloom if bloom in valid_levels else \"Unclear\")\n",
        "            except:\n",
        "                bloom_levels.append(\"Unclear\")\n",
        "        while len(bloom_levels) < len(exam_df):\n",
        "            bloom_levels.append(\"Unclear\")\n",
        "    except Exception as e:\n",
        "        print(f\"Bloom API Error: {e}\")\n",
        "        bloom_levels += [\"API Error\"] * len(batch_df)\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels[:len(exam_df)]\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Result ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification (Batched + Robust Parsing) ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Batched + Robust Parsing) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_blooms_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"])\n",
        "        for idx, label in parsed.items():\n",
        "            bloom_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Bloom API Error: {e}\")\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Result ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "FGOcOnMAQ1hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification (Batched + Robust Parsing) ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Batched + Robust Parsing) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_blooms_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"])\n",
        "        for idx, label in parsed.items():\n",
        "            bloom_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Bloom API Error: {e}\")\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Result ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "SgtKBkceRevZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification (Batched + Robust Parsing) ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Batched + Retry on 429 Errors) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):  # Smaller batch\n",
        "    success = False\n",
        "    retries = 3\n",
        "    wait_seconds = 15\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):  # Rate limit\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(wait_seconds)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Result ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "OWrvSfeNSpnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification (Batched + Robust Parsing) ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Batched + Retry on 429 Errors) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):  # Smaller batch\n",
        "    success = False\n",
        "    retries = 3\n",
        "    wait_seconds = 15\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):  # Rate limit\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(wait_seconds)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Result ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "COwAm-mYTYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Initial Pass) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):  # Smaller batch to reduce 429 errors\n",
        "    success = False\n",
        "    retries = 3\n",
        "    wait_seconds = 15\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):  # Rate limit\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(wait_seconds)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Retry Bloom Classification for Unclear ===\n",
        "unclear_bloom_df = exam_df[exam_df[\"bloom_level\"] == \"Unclear\"]\n",
        "if not unclear_bloom_df.empty:\n",
        "    print(f\"\\nRetrying Bloom classification for {len(unclear_bloom_df)} unclear items...\")\n",
        "    for batch_df in split_df_into_batches(unclear_bloom_df, 5):\n",
        "        success = False\n",
        "        retries = 2\n",
        "        while not success and retries > 0:\n",
        "            try:\n",
        "                prompt = build_blooms_batch_prompt(batch_df)\n",
        "                response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "                parsed = extract_labels(response.text, valid_bloom)\n",
        "                for idx, label in parsed.items():\n",
        "                    exam_df.at[idx, \"bloom_level\"] = label\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e):\n",
        "                    print(\"Quota limit reached during retry. Waiting...\")\n",
        "                    time.sleep(15)\n",
        "                    retries -= 1\n",
        "                else:\n",
        "                    print(f\"Retry Bloom API Error: {e}\")\n",
        "                    break\n",
        "\n",
        "# === Summary Output ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "mKn-b9svUwX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Initial Pass) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):  # Smaller batch to reduce 429 errors\n",
        "    success = False\n",
        "    retries = 3\n",
        "    wait_seconds = 15\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):  # Rate limit\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(wait_seconds)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Retry Bloom Classification for Unclear ===\n",
        "unclear_bloom_df = exam_df[exam_df[\"bloom_level\"] == \"Unclear\"]\n",
        "if not unclear_bloom_df.empty:\n",
        "    print(f\"\\nRetrying Bloom classification for {len(unclear_bloom_df)} unclear items...\")\n",
        "    for batch_df in split_df_into_batches(unclear_bloom_df, 5):\n",
        "        success = False\n",
        "        retries = 2\n",
        "        while not success and retries > 0:\n",
        "            try:\n",
        "                prompt = build_blooms_batch_prompt(batch_df)\n",
        "                response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "                parsed = extract_labels(response.text, valid_bloom)\n",
        "                for idx, label in parsed.items():\n",
        "                    exam_df.at[idx, \"bloom_level\"] = label\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e):\n",
        "                    print(\"Quota limit reached during retry. Waiting...\")\n",
        "                    time.sleep(15)\n",
        "                    retries -= 1\n",
        "                else:\n",
        "                    print(f\"Retry Bloom API Error: {e}\")\n",
        "                    break\n",
        "\n",
        "# === Summary Output ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "xcxFluwfVVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Initial Pass) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):  # Smaller batch to reduce 429 errors\n",
        "    success = False\n",
        "    retries = 3\n",
        "    wait_seconds = 15\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):  # Rate limit\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(wait_seconds)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Retry Bloom Classification for Unclear ===\n",
        "unclear_bloom_df = exam_df[exam_df[\"bloom_level\"] == \"Unclear\"]\n",
        "if not unclear_bloom_df.empty:\n",
        "    print(f\"\\nRetrying Bloom classification for {len(unclear_bloom_df)} unclear items...\")\n",
        "    for batch_df in split_df_into_batches(unclear_bloom_df, 5):\n",
        "        success = False\n",
        "        retries = 2\n",
        "        while not success and retries > 0:\n",
        "            try:\n",
        "                prompt = build_blooms_batch_prompt(batch_df)\n",
        "                response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "                parsed = extract_labels(response.text, valid_bloom)\n",
        "                for idx, label in parsed.items():\n",
        "                    exam_df.at[idx, \"bloom_level\"] = label\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e):\n",
        "                    print(\"Quota limit reached during retry. Waiting...\")\n",
        "                    time.sleep(15)\n",
        "                    retries -= 1\n",
        "                else:\n",
        "                    print(f\"Retry Bloom API Error: {e}\")\n",
        "                    break\n",
        "\n",
        "# === Summary Output ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "4E3yqnOWV5be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Delay to Prevent Quota Hit ===\n",
        "print(\"\\nWaiting for 45 seconds before starting Bloom's Taxonomy classification...\")\n",
        "time.sleep(45)\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Initial Pass) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):\n",
        "    success = False\n",
        "    retries = 3\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(15)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Retry Bloom Classification for Unclear ===\n",
        "unclear_bloom_df = exam_df[exam_df[\"bloom_level\"] == \"Unclear\"]\n",
        "if not unclear_bloom_df.empty:\n",
        "    print(f\"\\nRetrying Bloom classification for {len(unclear_bloom_df)} unclear items...\")\n",
        "    for batch_df in split_df_into_batches(unclear_bloom_df, 5):\n",
        "        success = False\n",
        "        retries = 2\n",
        "        while not success and retries > 0:\n",
        "            try:\n",
        "                prompt = build_blooms_batch_prompt(batch_df)\n",
        "                response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "                parsed = extract_labels(response.text, valid_bloom)\n",
        "                for idx, label in parsed.items():\n",
        "                    exam_df.at[idx, \"bloom_level\"] = label\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e):\n",
        "                    print(\"Quota limit reached during retry. Waiting...\")\n",
        "                    time.sleep(15)\n",
        "                    retries -= 1\n",
        "                else:\n",
        "                    print(f\"Retry Bloom API Error: {e}\")\n",
        "                    break\n",
        "\n",
        "# === Summary Output ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "qUkp33ddXjfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Delay to Prevent Quota Hit ===\n",
        "print(\"\\nWaiting for 45 seconds before starting Bloom's Taxonomy classification...\")\n",
        "time.sleep(45)\n",
        "\n",
        "# === Bloom’s Taxonomy Classification (Initial Pass) ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):\n",
        "    success = False\n",
        "    retries = 3\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(\"Quota limit reached. Retrying after delay...\")\n",
        "                time.sleep(15)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Retry Bloom Classification for Unclear ===\n",
        "unclear_bloom_df = exam_df[exam_df[\"bloom_level\"] == \"Unclear\"]\n",
        "if not unclear_bloom_df.empty:\n",
        "    print(f\"\\nRetrying Bloom classification for {len(unclear_bloom_df)} unclear items...\")\n",
        "    for batch_df in split_df_into_batches(unclear_bloom_df, 5):\n",
        "        success = False\n",
        "        retries = 2\n",
        "        while not success and retries > 0:\n",
        "            try:\n",
        "                prompt = build_blooms_batch_prompt(batch_df)\n",
        "                response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "                parsed = extract_labels(response.text, valid_bloom)\n",
        "                for idx, label in parsed.items():\n",
        "                    exam_df.at[idx, \"bloom_level\"] = label\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                if \"429\" in str(e):\n",
        "                    print(\"Quota limit reached during retry. Waiting...\")\n",
        "                    time.sleep(15)\n",
        "                    retries -= 1\n",
        "                else:\n",
        "                    print(f\"Retry Bloom API Error: {e}\")\n",
        "                    break\n",
        "\n",
        "# === Summary Output ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "0Fw5YzT9YPVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step 1: Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Step 2: WAIT before Bloom Classification ===\n",
        "print(\"\\n✅ Difficulty classification complete.\")\n",
        "print(\"⏳ Waiting 120 seconds before starting Bloom's taxonomy classification...\")\n",
        "time.sleep(120)\n",
        "\n",
        "# === Step 3: Bloom's Taxonomy Classification ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):\n",
        "    success = False\n",
        "    retries = 3\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(\"⚠️ Quota limit reached. Retrying after 15s...\")\n",
        "                time.sleep(15)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "pkBNq9VOcaLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Robust Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builders ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step 1: Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Step 2: WAIT before Bloom Classification ===\n",
        "print(\"\\n✅ Difficulty classification complete.\")\n",
        "print(\"⏳ Waiting 120 seconds before starting Bloom's taxonomy classification...\")\n",
        "time.sleep(120)\n",
        "\n",
        "# === Step 3: Bloom's Taxonomy Classification ===\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):\n",
        "    success = False\n",
        "    retries = 3\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(\"⚠️ Quota limit reached. Retrying after 15s...\")\n",
        "                time.sleep(15)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"evaluated_exam_gemini_revised.csv\", index=False)\n",
        "files.download(\"evaluated_exam_gemini_revised.csv\")\n"
      ],
      "metadata": {
        "id": "fAaCKydMd1VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload CSV Files ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Configuration ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "config = GenerationConfig(temperature=0.0, top_p=1.0, top_k=1)\n",
        "\n",
        "# === Semantic Matching ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === Prompt Builder ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy\n",
        "- Medium\n",
        "- Difficult\n",
        "\n",
        "Classify each MCQ in this format:\n",
        "Q[ID]: [Level]\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Label Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    results = {}\n",
        "    for line in text.strip().splitlines():\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Run Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for i in range(0, len(exam_df), 10):\n",
        "    batch_df = exam_df.iloc[i:i+10]\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Output Summary ===\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "\n",
        "# === Save Output ===\n",
        "exam_df.to_csv(\"difficulty_output.csv\", index=False)\n",
        "files.download(\"difficulty_output.csv\")\n"
      ],
      "metadata": {
        "id": "N9njQwgijLeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload CSV Files ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Configuration ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "config = GenerationConfig(temperature=0.0, top_p=1.0, top_k=1)\n",
        "\n",
        "# === Semantic Matching ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === Prompt Builder ===\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy\n",
        "- Medium\n",
        "- Difficult\n",
        "\n",
        "Classify each MCQ in this format:\n",
        "Q[ID]: [Level]\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Label Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    results = {}\n",
        "    for line in text.strip().splitlines():\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Run Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for i in range(0, len(exam_df), 10):\n",
        "    batch_df = exam_df.iloc[i:i+10]\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Output Summary ===\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "\n",
        "# === Save Output ===\n",
        "exam_df.to_csv(\"difficulty_output.csv\", index=False)\n",
        "files.download(\"difficulty_output.csv\")\n"
      ],
      "metadata": {
        "id": "mUrQkn56kjRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai sentence-transformers faiss-cpu pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload CSVs ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"grade9Q1.csv\", encoding=\"ISO-8859-1\")\n",
        "textbook_df = pd.read_csv(\"biologyg9.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Setup Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Semantic Search Setup ===\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "text_chunks = textbook_df[\"Text\"].tolist()\n",
        "chunk_embeddings = embedder.encode(text_chunks, convert_to_tensor=True)\n",
        "\n",
        "matched_units, matched_sections = [], []\n",
        "threshold = 0.6\n",
        "for question in exam_df[\"question_text\"]:\n",
        "    q_embed = embedder.encode(question, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(q_embed, chunk_embeddings)\n",
        "    best_idx = similarities.argmax().item()\n",
        "    best_score = similarities[0][best_idx].item()\n",
        "    if best_score >= threshold:\n",
        "        matched_units.append(textbook_df.iloc[best_idx][\"Unit\"])\n",
        "        matched_sections.append(textbook_df.iloc[best_idx][\"Section\"])\n",
        "    else:\n",
        "        matched_units.append(\"No Match\")\n",
        "        matched_sections.append(\"No Match\")\n",
        "\n",
        "exam_df[\"matched_unit\"] = matched_units\n",
        "exam_df[\"matched_section\"] = matched_sections\n",
        "\n",
        "# === MCQ Format Validation ===\n",
        "exam_df[\"option_count_valid\"] = exam_df.apply(\n",
        "    lambda row: all(pd.notnull(row[f\"option_{i}\"]) for i in range(1, 5)), axis=1\n",
        ")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Helper Functions ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "def build_difficulty_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are a biology teacher assessing the difficulty of multiple-choice questions.\n",
        "\n",
        "Difficulty Levels:\n",
        "- Easy: Factual recall or basic recognition\n",
        "- Medium: Requires explanation or understanding\n",
        "- Difficult: Requires analysis, synthesis, or reasoning\n",
        "\n",
        "Classify each MCQ with one word: Easy, Medium, or Difficult.\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Medium\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step 1: Difficulty Classification ===\n",
        "difficulty_levels = [\"Unclear\"] * len(exam_df)\n",
        "for batch_df in split_df_into_batches(exam_df, 10):\n",
        "    try:\n",
        "        prompt = build_difficulty_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, [\"Easy\", \"Medium\", \"Difficult\"])\n",
        "        for idx, label in parsed.items():\n",
        "            difficulty_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Difficulty API Error: {e}\")\n",
        "\n",
        "exam_df[\"difficulty\"] = difficulty_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Coverage by Section ===\")\n",
        "print(exam_df[\"matched_section\"].value_counts())\n",
        "print(\"\\n=== Difficulty Distribution ===\")\n",
        "print(exam_df[\"difficulty\"].value_counts())\n",
        "print(\"\\n=== Invalid MCQs (≠ 4 options) ===\")\n",
        "print(len(exam_df[~exam_df[\"option_count_valid\"]]))\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"difficulty_only.csv\", index=False)\n",
        "files.download(\"difficulty_only.csv\")\n"
      ],
      "metadata": {
        "id": "YRxWs5_al5rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload Difficulty-Classified File ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"difficulty_only.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "config = GenerationConfig(temperature=0.0, top_p=1.0, top_k=1)\n",
        "\n",
        "# === Helper Functions ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step: Bloom's Taxonomy Classification ===\n",
        "print(\"\\n⏳ Starting Bloom’s taxonomy classification after delay...\")\n",
        "time.sleep(60)\n",
        "\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):\n",
        "    success = False\n",
        "    retries = 3\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(\"⚠️ Quota limit reached. Retrying after 15s...\")\n",
        "                time.sleep(15)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"bloom_only.csv\", index=False)\n",
        "files.download(\"bloom_only.csv\")\n"
      ],
      "metadata": {
        "id": "GfCPTW10nPJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Upload Difficulty-Classified File ===\n",
        "uploaded = files.upload()\n",
        "exam_df = pd.read_csv(\"difficulty_only.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "config = GenerationConfig(temperature=0.0, top_p=1.0, top_k=1)\n",
        "\n",
        "# === Helper Functions ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in the format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step: Bloom's Taxonomy Classification ===\n",
        "print(\"\\n⏳ Starting Bloom’s taxonomy classification after delay...\")\n",
        "time.sleep(60)\n",
        "\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 5):\n",
        "    success = False\n",
        "    retries = 3\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            prompt = build_blooms_batch_prompt(batch_df)\n",
        "            response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "            parsed = extract_labels(response.text, valid_bloom)\n",
        "            for idx, label in parsed.items():\n",
        "                bloom_levels[idx] = label\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(\"⚠️ Quota limit reached. Retrying after 15s...\")\n",
        "                time.sleep(15)\n",
        "                retries -= 1\n",
        "            else:\n",
        "                print(f\"Bloom API Error: {e}\")\n",
        "                break\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"bloom_only.csv\", index=False)\n",
        "files.download(\"bloom_only.csv\")\n"
      ],
      "metadata": {
        "id": "Q7diZHmVoIP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload Difficulty-Classified File ===\n",
        "uploaded = files.upload()  # Upload difficulty_only.csv\n",
        "exam_df = pd.read_csv(\"difficulty_only.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Stable Generation Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,  # No randomness\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builder (Bloom) ===\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step: Bloom's Taxonomy Classification ===\n",
        "print(\"⏳ Starting stable Bloom’s taxonomy classification...\")\n",
        "\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 10):  # consistent batching\n",
        "    try:\n",
        "        prompt = build_blooms_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, valid_bloom)\n",
        "        for idx, label in parsed.items():\n",
        "            bloom_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Bloom API Error: {e}\")\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"bloom_stable.csv\", index=False)\n",
        "files.download(\"bloom_stable.csv\")\n"
      ],
      "metadata": {
        "id": "iodS0V86psEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload Difficulty-Classified File ===\n",
        "uploaded = files.upload()  # Upload difficulty_only.csv\n",
        "exam_df = pd.read_csv(\"difficulty_only.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Stable Generation Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,  # No randomness\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builder (Bloom) ===\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step: Bloom's Taxonomy Classification ===\n",
        "print(\"⏳ Starting stable Bloom’s taxonomy classification...\")\n",
        "\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 10):  # consistent batching\n",
        "    try:\n",
        "        prompt = build_blooms_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, valid_bloom)\n",
        "        for idx, label in parsed.items():\n",
        "            bloom_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Bloom API Error: {e}\")\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"bloom_stable.csv\", index=False)\n",
        "files.download(\"bloom_stable.csv\")\n"
      ],
      "metadata": {
        "id": "ydP5CBpZxEEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install Required Libraries ===\n",
        "!pip install -q -U google-generativeai pandas\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.colab import files\n",
        "import re\n",
        "\n",
        "# === Upload Difficulty-Classified File ===\n",
        "uploaded = files.upload()  # Upload difficulty_only.csv\n",
        "exam_df = pd.read_csv(\"difficulty_only.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# === Gemini Setup ===\n",
        "genai.configure(api_key=\"AIzaSyCGgwigTnwX7CSJZfdvlCAdZ46KQTyD6XI\")  # Replace with your actual Gemini API key\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === Stable Generation Configuration ===\n",
        "config = GenerationConfig(\n",
        "    temperature=0.0,  # No randomness\n",
        "    top_p=1.0,\n",
        "    top_k=1\n",
        ")\n",
        "\n",
        "# === Helper: Split DataFrame into Batches ===\n",
        "def split_df_into_batches(df, batch_size):\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        yield df.iloc[i:i + batch_size]\n",
        "\n",
        "# === Output Parser ===\n",
        "def extract_labels(text, valid_labels):\n",
        "    lines = text.strip().splitlines()\n",
        "    results = {}\n",
        "    for line in lines:\n",
        "        match = re.match(r\"Q(\\d+):?\\s*(\\w+)\", line.strip())\n",
        "        if match:\n",
        "            idx, label = int(match.group(1)), match.group(2)\n",
        "            results[idx] = label if label in valid_labels else \"Unclear\"\n",
        "    return results\n",
        "\n",
        "# === Prompt Builder (Bloom) ===\n",
        "def build_blooms_batch_prompt(df):\n",
        "    header = \"\"\"\n",
        "You are an educational evaluator. Classify each biology MCQ using Bloom's Revised Taxonomy.\n",
        "\n",
        "Levels:\n",
        "- Remember\n",
        "- Understand\n",
        "- Apply\n",
        "- Analyze\n",
        "- Evaluate\n",
        "- Create\n",
        "\n",
        "Respond in this format:\n",
        "Q[ID]: [Level]\n",
        "\n",
        "Example:\n",
        "Q23: Apply\n",
        "\"\"\"\n",
        "    body = \"\"\n",
        "    for idx, row in df.iterrows():\n",
        "        body += f\"\\nQ{idx}: {row['question_text']}\\n\"\n",
        "        body += f\"A. {row['option_1']}\\nB. {row['option_2']}\\nC. {row['option_3']}\\nD. {row['option_4']}\\n\"\n",
        "    return header + body\n",
        "\n",
        "# === Step: Bloom's Taxonomy Classification ===\n",
        "print(\"⏳ Starting stable Bloom’s taxonomy classification...\")\n",
        "\n",
        "bloom_levels = [\"Unclear\"] * len(exam_df)\n",
        "valid_bloom = [\"Remember\", \"Understand\", \"Apply\", \"Analyze\", \"Evaluate\", \"Create\"]\n",
        "\n",
        "for batch_df in split_df_into_batches(exam_df, 10):  # consistent batching\n",
        "    try:\n",
        "        prompt = build_blooms_batch_prompt(batch_df)\n",
        "        response = gemini_model.generate_content(prompt, generation_config=config)\n",
        "        parsed = extract_labels(response.text, valid_bloom)\n",
        "        for idx, label in parsed.items():\n",
        "            bloom_levels[idx] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Bloom API Error: {e}\")\n",
        "\n",
        "exam_df[\"bloom_level\"] = bloom_levels\n",
        "\n",
        "# === Final Output Summary ===\n",
        "print(\"\\n=== Bloom’s Distribution ===\")\n",
        "print(exam_df[\"bloom_level\"].value_counts())\n",
        "\n",
        "# === Save Final Output ===\n",
        "exam_df.to_csv(\"bloom_stable.csv\", index=False)\n",
        "files.download(\"bloom_stable.csv\")\n"
      ],
      "metadata": {
        "id": "h5eGUMMVxWac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}